<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Cool Object Detector with Voice</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.6.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <style>
    body {
      margin: 0;
      overflow: hidden;
      background: #111;
      font-family: 'Segoe UI', sans-serif;
    }
    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
    }
    #label {
      position: absolute;
      top: 20px;
      left: 20px;
      background: rgba(255, 255, 255, 0.1);
      padding: 10px 18px;
      border-radius: 12px;
      color: #fff;
      font-size: 18px;
      backdrop-filter: blur(10px);
      border: 1px solid rgba(255, 255, 255, 0.2);
    }
  </style>
</head>
<body>
  <video id="webcam" width="640" height="480" autoplay muted playsinline></video>
  <canvas id="canvas" width="640" height="480"></canvas>
  <div id="label">Loading model...</div>

  <script>
    const video = document.getElementById('webcam');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const label = document.getElementById('label');

    let lastSpoken = '';
    let speakCooldown = false;

    function speak(text) {
      if (speakCooldown || text === lastSpoken) return;
      const msg = new SpeechSynthesisUtterance("I see " + text);
      msg.lang = 'en-US';
      speechSynthesis.speak(msg);
      lastSpoken = text;
      speakCooldown = true;
      setTimeout(() => speakCooldown = false, 2500);
    }

    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      return new Promise(resolve => {
        video.onloadedmetadata = () => resolve(video);
      });
    }

    async function startDetection() {
      await setupCamera();
      const model = await cocoSsd.load();
      label.textContent = "Model loaded. Detecting...";

      function drawRoundedRect(x, y, width, height, radius) {
        ctx.beginPath();
        ctx.moveTo(x + radius, y);
        ctx.lineTo(x + width - radius, y);
        ctx.quadraticCurveTo(x + width, y, x + width, y + radius);
        ctx.lineTo(x + width, y + height - radius);
        ctx.quadraticCurveTo(x + width, y + height, x + width - radius, y + height);
        ctx.lineTo(x + radius, y + height);
        ctx.quadraticCurveTo(x, y + height, x, y + height - radius);
        ctx.lineTo(x, y + radius);
        ctx.quadraticCurveTo(x, y, x + radius, y);
        ctx.closePath();
      }

      async function detect() {
        const predictions = await model.detect(video);
        ctx.clearRect(0, 0, canvas.width, canvas.height);

        let spokenLabels = [];

        predictions.forEach(pred => {
          if (pred.score > 0.65) {
            const [x, y, width, height] = pred.bbox;
            drawRoundedRect(x, y, width, height, 10);
            ctx.strokeStyle = 'rgba(255, 0, 0, 0.9)';
            ctx.lineWidth = 3;
            ctx.shadowColor = 'red';
            ctx.shadowBlur = 10;
            ctx.stroke();
            ctx.shadowBlur = 0;

            ctx.fillStyle = 'red';
            ctx.font = '16px Arial';
            ctx.fillText(`${pred.class} (${Math.round(pred.score * 100)}%)`, x + 5, y > 20 ? y - 10 : y + 20);

            spokenLabels.push(pred.class);
          }
        });

        if (spokenLabels.length > 0) {
          speak(spokenLabels.join(' and '));
          label.textContent = `Detected: ${spokenLabels.join(', ')}`;
        } else {
          label.textContent = "Nothing detected";
        }

        requestAnimationFrame(detect);
      }

      detect();
    }

    startDetection();
  </script>
</body>
</html>
